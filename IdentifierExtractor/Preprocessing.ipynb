{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = 'tokens'\n",
    "_DATA_DIR = f'../data/{DATASET_NAME}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_json(_DATA_DIR + '/tokens.json')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional - filter out rows where the method name doesnt contain any of the chosen subtokens\n",
    "# classes = { 0: 'train', 1: 'save', 2: 'process', 3: 'forward', 4: 'predict' }\n",
    "\n",
    "# df = df[df.method_name.str.contains(\"|\".join(classes.values()))]\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign categories based on method name\n",
    "# df['category'] = df.method_name.map(lambda x: np.array([x.find(s) for s in classes.values()]).argmax())\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.groupby('category').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def camel_case_split(identifier, joinToken):\n",
    "    matches = re.finditer(\n",
    "        '.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)',\n",
    "        identifier,\n",
    "    )\n",
    "    return f'{joinToken}'.join([m.group(0).lower() for m in matches])\n",
    "\n",
    "def snake_case_split(identifier, joinToken):\n",
    "    return f'{joinToken}'.join([x for x in identifier.split('_') if x != ''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_processed = df.copy()\n",
    "\n",
    "df['tokens'] = df['tokens'].apply(lambda x: list(np.unique(x))) # Dont store duplicates\n",
    "df_processed = df_processed[~df_processed.tokens.str.len().eq(0)]\n",
    "df_processed['tokens'] = df_processed['tokens'].apply(lambda x: [snake_case_split(s, ',') for s in x] )\n",
    "df_processed['tokens'] = df_processed['tokens'].apply(lambda x: \",\".join([camel_case_split(s, ',') for s in x]))\n",
    "\n",
    "# split camel/snake case method names\n",
    "df_processed['method_name'] = df_processed.method_name.map(lambda x: snake_case_split(x, '|'))\n",
    "df_processed['method_name'] = df_processed.method_name.map(lambda x: camel_case_split(x, '|'))\n",
    "\n",
    "df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = df_processed['tokens'].str.split(',').values\n",
    "num_tokens_per_method = [len(l) for l in tokens]\n",
    "print(f\"Max number of tokens in method {np.max(num_tokens_per_method)}\\nMin number of tokens in a method {np.min(num_tokens_per_method)}\\nAverage number of tokens per method {np.mean(num_tokens_per_method):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of unique method names: {len(np.unique(df_processed.method_name.values))}\\nNumber of unique tokens {len(np.unique(tokens))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_processed.drop(columns=['file'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Partition into sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size, val_size, test_size = 0.9, 0.05, 0.05\n",
    "train, remainder = train_test_split(df_processed, test_size=(1-train_size), shuffle=True)\n",
    "validate, test =  train_test_split(remainder, test_size=test_size/(test_size + val_size))\n",
    "\n",
    "print(f\"{len(train)} train samples\\n{len(validate)} validation samples\\n{len(test)} test samples\")\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "train.to_csv(_DATA_DIR+'/train.csv', encoding='utf-8', sep=\" \", index=False, header=None, quoting = csv.QUOTE_NONE, escapechar = ' ')\n",
    "validate.to_csv(_DATA_DIR+'/val.csv', encoding='utf-8', sep=\" \", index=False, header=None, quoting = csv.QUOTE_NONE, escapechar = ' ')\n",
    "test.to_csv(_DATA_DIR+'/test.csv', encoding='utf-8', sep=\" \", index=False, header=None, quoting = csv.QUOTE_NONE, escapechar = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (scc)",
   "language": "python",
   "name": "scc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
