{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "    \n",
    "import ast\n",
    "import csv\n",
    "import json\n",
    "import common\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'cubert'\n",
    "max_tokens = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inter = pd.read_json('./data/output.jsonl', lines=True)\n",
    "df_inter.columns = ['json_element']\n",
    "df_inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inter = df_inter['json_element'].apply(json.loads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.json_normalize(df_inter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tokens = df.tokens.map(lambda x: [s.replace(' ', '') for s in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.tokens.map(len) <= max_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write frequency of targets to file\n",
    "df.method_name.value_counts().reset_index().to_csv(f'./data/{dataset_name}.histo.tgt.c2v', sep=\" \", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram for tokens\n",
    "df.tokens.explode('tokens').value_counts().reset_index().to_csv(f'./data/{dataset_name}.histo.ori.c2v', sep=\" \", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_and_stringify(tokens):\n",
    "    csv_padding = \" \" * (max_tokens - len(tokens))\n",
    "    stringified_tokens = \" \".join([f\"'{s}'\" for s in tokens])\n",
    "    padded_stringified_tokens = stringified_tokens + csv_padding\n",
    "    return padded_stringified_tokens\n",
    "\n",
    "def save_dictionaries(dataset_name, token_to_count, target_to_count,\n",
    "                      num_training_examples):\n",
    "    save_dict_file_path = './data/{}.dict.c2v'.format(dataset_name)\n",
    "    with open(save_dict_file_path, 'wb') as file:\n",
    "        pickle.dump(token_to_count, file)\n",
    "        pickle.dump(target_to_count, file)\n",
    "        pickle.dump(num_training_examples, file)\n",
    "        print('Dictionaries saved to: {}'.format(save_dict_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['stringified_tokens'] = df.tokens.map(pad_and_stringify) # Stringify for csv\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.2, shuffle=True)\n",
    "print(f\"{len(train)} train samples\\n{len(test)} test samples\")\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training_examples = len(train)\n",
    "target_vocab_size = len(train.method_name.unique())\n",
    "token_vocab_size = len(df.tokens.explode('tokens').unique())\n",
    "print(f\"Unique method names: {target_vocab_size}\\nUnique tokens: {token_vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['method_name', 'stringified_tokens']].to_csv(f'./data/{dataset_name}.train.c2v', encoding='utf-8', sep=\" \", index=False, header=None, quoting = csv.QUOTE_NONE, escapechar = ',')\n",
    "test[['method_name', 'stringified_tokens']].to_csv(f'./data/{dataset_name}.test.c2v', encoding='utf-8', sep=\" \", index=False, header=None, quoting = csv.QUOTE_NONE, escapechar = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " word_histogram_data = common.common.load_vocab_from_histogram(f'./data/{dataset_name}.histo.ori.c2v', start_from=1,\n",
    "                                                                  max_size=int(\n",
    "                                                                      token_vocab_size),\n",
    "                                                                  return_counts=True)\n",
    "_, _, _, word_to_count = word_histogram_data\n",
    "_, _, _, target_to_count = common.common.load_vocab_from_histogram(f'./data/{dataset_name}.histo.tgt.c2v', start_from=1,\n",
    "                                                                   max_size=int(\n",
    "                                                                       target_vocab_size),\n",
    "                                                                   return_counts=True)\n",
    "\n",
    "save_dictionaries(dataset_name=dataset_name, token_to_count=word_to_count, target_to_count=target_to_count,\n",
    "                  num_training_examples=num_training_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (scc)",
   "language": "python",
   "name": "scc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
