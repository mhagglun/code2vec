{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import os\n",
    "import csv\n",
    "import tikzplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import kmapper as km\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from kmapper import jupyter\n",
    "from itertools import cycle\n",
    "from sklearn import metrics\n",
    "from collections import OrderedDict\n",
    "from validclust.indices import dunn\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, OPTICS\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.models import Model\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "pd.options.mode.chained_assignment = None\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"pathcontext\"\n",
    "if not os.path.exists(f\"./results/tables/{DATASET_NAME}\"):\n",
    "    os.makedirs(f\"./results/tables/{DATASET_NAME}\")\n",
    "if not os.path.exists(f\"./results/figures/{DATASET_NAME}\"):    \n",
    "    os.makedirs(f\"./results/figures/{DATASET_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load method names\n",
    "method_names = pd.read_csv(f'./data/{DATASET_NAME}/{DATASET_NAME}.test.c2v', sep=\" \", dtype=str).iloc[:, 0]\n",
    "predicted_method_names = pd.read_csv(f'./data/{DATASET_NAME}/{DATASET_NAME}.test.c2v.predicted_names', sep=\" \", dtype=str).iloc[:, 0]\n",
    "print(len(method_names))\n",
    "\n",
    "# Load code vectors\n",
    "vectors = pd.read_csv(f'./data/{DATASET_NAME}/{DATASET_NAME}.test.c2v.vectors', sep=\" \", header=None)\n",
    "print(len(vectors))\n",
    "\n",
    "# Load method embeddings\n",
    "target_embeddings = f'./data/{DATASET_NAME}/targets.txt'\n",
    "t2v = KeyedVectors.load_word2vec_format(target_embeddings, binary=False)\n",
    "target_vocab = t2v.vocab.keys()\n",
    "\n",
    "code_vector_dim = vectors.iloc[0].shape[0]\n",
    "embedding_dim = 128\n",
    "print(len(method_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge vectors and labels\n",
    "codevector_df = vectors.assign(method_name=method_names)\n",
    "codevector_df['predicted_method_name'] = predicted_method_names\n",
    "\n",
    "# Drop method names which are not contained in the embedding space\n",
    "codevector_df = codevector_df[codevector_df.method_name.isin(target_vocab)]\n",
    "method_names = method_names[method_names.isin(target_vocab)]\n",
    "# method_names\n",
    "\n",
    "codevector_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe for method name embeddings\n",
    "df_inter = pd.DataFrame([{\"predicted_method_name\": name, \"embedding\": t2v.get_vector(name)} for name in predicted_method_names])\n",
    "\n",
    "embeddings = df_inter['embedding'].apply(pd.Series)\n",
    "embeddings = embeddings.rename(columns = lambda x : 'feat_' + str(x))\n",
    "\n",
    "method_name_embedding_df = pd.concat([embeddings[:], df_inter[:], method_names], axis=1)\n",
    "method_name_embedding_df = method_name_embedding_df.rename(columns={method_name_embedding_df.columns[-1]: 'method_name'})\n",
    "method_name_embedding_df.drop(columns=[\"embedding\"], inplace=True)\n",
    "method_name_embedding_df.dropna(inplace=True)\n",
    "method_name_embedding_df.reset_index(drop=True, inplace=True)\n",
    "method_name_embedding_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(method_names))\n",
    "print(len(codevector_df))\n",
    "print(len(method_name_embedding_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = { 0: 'train', 1: 'save', 2: 'process', 3: 'forward', 4: 'predict' }\n",
    "\n",
    "# Optional, filter out methods which do not contain any of the chosen classes\n",
    "method_name_embedding_df = method_name_embedding_df[method_name_embedding_df.method_name.str.contains(\"|\".join(classes.values()))]\n",
    "codevector_df = codevector_df[codevector_df.method_name.str.contains(\"|\".join(classes.values()))]\n",
    "codevector_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Assign categories based on method name\n",
    "method_name_embedding_df['category'] = method_name_embedding_df.method_name.map(lambda x: np.array([x.find(s) for s in classes.values()]).argmax())\n",
    "codevector_df['category'] = codevector_df.method_name.map(lambda x: np.array([x.find(s) for s in classes.values()]).argmax())\n",
    "method_name_embedding_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = method_name_embedding_df.apply(lambda x: x['predicted_method_name'] in (x['method_name']), axis=1)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Percentage of instances with matching subtokens: {sum(matches)} / {len(matches)} = {sum(matches)/len(matches)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_name_embedding_df.groupby('category').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codevector_df.groupby('category').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose a subset of samples\n",
    "We'll only consider a subset of samples for visualization. This is done by taking an equal number of instances from each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_name_subset_df = method_name_embedding_df.sample(n=1000, random_state=7).reset_index(drop=True)\n",
    "codevector_subset_df = codevector_df.sample(n=1000, random_state=7).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_name_subset_df.groupby('category').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codevector_subset_df.groupby('category').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the \"default\" high dim features\n",
    "codevector_features = codevector_subset_df.iloc[:, 0:code_vector_dim].values\n",
    "codevector_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_name_features = method_name_subset_df.iloc[:, 0:embedding_dim].values\n",
    "method_name_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare methods for computing metrics and visualizing clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Use tSNE\n",
    "tsne = TSNE(n_components=3, verbose=1, perplexity=perplexity, n_iter=3000)\n",
    "method_name_tsne = tsne.fit_transform(method_name_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=3, verbose=1, perplexity=perplexity, n_iter=3000)\n",
    "codevector_tsne = tsne.fit_transform(codevector_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {'Method Name Embedding': {'DataFrame': method_name_subset_df, 'Features': method_name_features, 'TSNE': method_name_tsne}, 'Code Vectors': {'DataFrame': codevector_subset_df, 'Features': codevector_features, 'TSNE': codevector_tsne}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(estimator, dataframe, features, predicted_labels):\n",
    "\n",
    "    distances = pairwise_distances(features)\n",
    "    \n",
    "    results = {}\n",
    "    results['estimator'] = estimator.__class__.__name__\n",
    "    results['homogeneity_score'] = metrics.homogeneity_score(dataframe['category'], predicted_labels)\n",
    "    results['completeness_score'] = metrics.completeness_score(dataframe['category'], predicted_labels)\n",
    "    results['v_measure_score'] = metrics.v_measure_score(dataframe['category'], predicted_labels)\n",
    "    results['adjusted_rand_score'] = metrics.adjusted_rand_score(dataframe['category'], predicted_labels)\n",
    "    results['adjusted_mutual_info_score'] = metrics.adjusted_mutual_info_score(dataframe['category'], predicted_labels)\n",
    "    results['average_jaccard_score'] = np.mean(metrics.jaccard_score(dataframe['category'], predicted_labels, average=None))\n",
    "    results['dunn_index'] = dunn(distances, predicted_labels)\n",
    "    \n",
    "    if len(np.unique(predicted_labels)) == 1 or len(np.unique(predicted_labels)) == len(features):\n",
    "        results['silhouette_score'] = -1\n",
    "    else:\n",
    "        results['silhouette_score'] = metrics.silhouette_score(features, predicted_labels, metric='sqeuclidean')\n",
    "    return results\n",
    "\n",
    "def plot_clusters(estimator, metrics):\n",
    "    estimator_name = estimator.__class__.__name__\n",
    "    \n",
    "    \n",
    "    code_vector_labels = metrics['Code Vectors']['labels']\n",
    "    code_vector_metrics = metrics['Code Vectors']['metrics']\n",
    "    \n",
    "    method_name_labels = metrics['Method Name Embedding']['labels']\n",
    "    method_name_metrics = metrics['Method Name Embedding']['metrics']\n",
    "    \n",
    "    fig = plt.figure(figsize=(28,8))\n",
    "    \n",
    "    for i, key in enumerate(metrics):\n",
    "        labels =  metrics[key]['labels']\n",
    "        metric = metrics[key]['metrics']\n",
    "        dataframe = datasets[key]['DataFrame']\n",
    "        tnse_projection = datasets[key]['TSNE']\n",
    "                         \n",
    "        k = len(np.unique(labels))\n",
    "        # Print metrics\n",
    "        print(key)\n",
    "        print('Number of clusters: %d' % k)\n",
    "        print(\"Homogeneity: %0.3f\" % metric['homogeneity_score'])\n",
    "        print(\"Completeness: %0.3f\" % metric['completeness_score'])\n",
    "        print(\"V-measure: %0.3f\" % metric['v_measure_score'])\n",
    "        print(\"Adjusted Rand Index: %0.3f\"\n",
    "              % metric['adjusted_rand_score'])\n",
    "        print(\"Adjusted Mutual Information: %0.3f\"\n",
    "              % metric['adjusted_mutual_info_score'])\n",
    "        print(\"Mean Jaccard Coefficient: %s\"\n",
    "              % metric['average_jaccard_score'])\n",
    "        print(\"Silhouette Coefficient: %0.3f\"\n",
    "              % metric['silhouette_score'])\n",
    "        print(\"Dunn Index: %0.3f\\n\"\n",
    "              % metric['dunn_index'])\n",
    "    \n",
    "        # Visualize clusters with tSNE\n",
    "        ax1 = fig.add_subplot(int(f\"14{2*i + 1}\"), projection='3d')\n",
    "        ax1.set_title(f'{estimator_name} (k={k}) Clusters ({key})')\n",
    "        colors = cm.tab10(np.linspace(0, 1, k))\n",
    "        if estimator_name == 'OPTICS':\n",
    "            ax1.scatter(tnse_projection[:, 0], tnse_projection[:, 1], tnse_projection[:, 2], c='k', marker='+', alpha=0.1)\n",
    "        for klass, color in zip(range(0, k), colors):\n",
    "            Xk = tnse_projection[labels == klass]\n",
    "            ax1.scatter(Xk[:, 0], Xk[:, 1], Xk[:, 2], color=color, alpha=0.3, label=f'Cluster ID {klass+1}')\n",
    "\n",
    "#         ax1.legend(title='Cluster IDs', bbox_to_anchor=(1.3, 0.5), loc='right', fancybox=True)\n",
    "        h,l = ax1.get_legend_handles_labels()\n",
    "        plt.legend(h[:10], l[:10], title='Cluster IDs', bbox_to_anchor=(1.3, 0.5), loc='right', fancybox=True)\n",
    "        \n",
    "        ax2 = fig.add_subplot(int(f\"14{2*(i+1)}\"), projection='3d')\n",
    "        ax2.set_title(f'Method names as labels ({key})')\n",
    "        colors = cycle('bgrcmykbgrcmykbgrcmykbgrcmyk')\n",
    "        for klass, color in zip(range(0, len(classes)), colors):\n",
    "            Xk = tnse_projection[dataframe['category'] == klass]\n",
    "            ax2.scatter(Xk[:, 0], Xk[:, 1],  Xk[:, 2], c=color, alpha=0.3, label=classes[klass])\n",
    "        ax2.legend(title='Method name', bbox_to_anchor=(1.3, 0.5), loc='right', fancybox=True)\n",
    "    \n",
    "    fig.subplots_adjust(wspace=0.1)\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(f\"./results/figures/{DATASET_NAME}/code2vec_{DATASET_NAME}_{estimator.__class__.__name__}.pdf\")\n",
    "    \n",
    "\n",
    "def cv_silhouette_scorer(estimator, X):\n",
    "    estimator.fit(X)\n",
    "    if (estimator.__class__.__name__ == \"OPTICS\"):\n",
    "        cluster_labels = estimator.labels_[estimator.ordering_]\n",
    "    else:\n",
    "        cluster_labels = estimator.labels_\n",
    "    num_labels = len(set(cluster_labels))\n",
    "    num_samples = len(X)\n",
    "    if num_labels == 1 or num_labels == num_samples:\n",
    "        return -1\n",
    "    else:\n",
    "        return metrics.silhouette_score(X, cluster_labels, metric='sqeuclidean')\n",
    "\n",
    "def cv_dunn_scorer(estimator, X):\n",
    "    estimator.fit(X)\n",
    "    if (estimator.__class__.__name__ == \"OPTICS\"):\n",
    "        cluster_labels = estimator.labels_[estimator.ordering_]\n",
    "    else:\n",
    "        cluster_labels = estimator.labels_\n",
    "    num_labels = len(set(cluster_labels))\n",
    "    num_samples = len(X)\n",
    "    if num_labels == 1 or num_labels == 0 or num_labels == num_samples:\n",
    "        return 0\n",
    "    else:\n",
    "        return dunn_fast(X, cluster_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fitting and hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = OrderedDict()\n",
    "estimator_metrics = OrderedDict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centroid-based clustering using K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Apply K-means\n",
    "search_params = {'n_clusters': np.arange(2,11)}\n",
    "\n",
    "kmeans_results = {}\n",
    "\n",
    "for key in datasets:\n",
    "    df = datasets[key]['DataFrame']\n",
    "    features = datasets[key]['Features']\n",
    "    \n",
    "    cv = [(slice(None), slice(None))] # Disable cv, only want grid search\n",
    "    gs = GridSearchCV(estimator=KMeans(random_state=0), param_grid=search_params, \n",
    "                      scoring=cv_silhouette_scorer, cv=cv, n_jobs=-1)\n",
    "\n",
    "    res = gs.fit(X=features, y=None)\n",
    "    \n",
    "    max_score = np.max(res.cv_results_['mean_test_score'])\n",
    "    ind = np.argmax(res.cv_results_['mean_test_score'])\n",
    "    k = search_params['n_clusters'][ind]\n",
    "\n",
    "    print(f\"Best validation score {max_score:.3f} achieved with {k} clusters\")\n",
    "    kmeans_estimator = KMeans(n_clusters=k, random_state=0)\n",
    "    kmeans_name = kmeans_estimator.__class__.__name__\n",
    "\n",
    "    kmeans = kmeans_estimator.fit(features)\n",
    "    kmeans_metrics = calculate_metrics(kmeans_estimator, df, features, kmeans.labels_)\n",
    "    kmeans_results[key] = {'labels': kmeans.labels_, 'metrics': kmeans_metrics}\n",
    "    \n",
    "    estimators[f\"{kmeans_name} on {key}\"] = {'estimator': kmeans_estimator, 'score':  max_score, 'method representation': key }\n",
    "    estimator_metrics[f\"{kmeans_name} on {key}\"] = {'Method Representation': key, 'Estimator': kmeans_name, 'Dunn Index': kmeans_metrics['dunn_index'], 'Silhouette Score': kmeans_metrics['silhouette_score'], 'Adjusted Rand Index': kmeans_metrics['adjusted_rand_score']}\n",
    "\n",
    "\n",
    "plot_clusters(kmeans, kmeans_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Density-Based Clustering using OPTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_params = {'cluster_method': ['xi', 'dbscan'], 'xi': np.linspace(0,1,11), 'min_samples': [2,5,10,15,20,25,30], }\n",
    "search_params = {'xi': np.linspace(0.1,1,10), 'min_samples': [2,5,10,15,20,25,30]}\n",
    "optics_results = {}\n",
    "\n",
    "for key in datasets:\n",
    "    df = datasets[key]['DataFrame']\n",
    "    features = datasets[key]['Features']\n",
    "    \n",
    "    cv = [(slice(None), slice(None))] # Disable cv, only want grid search\n",
    "    gs = GridSearchCV(estimator=OPTICS(cluster_method='xi'), param_grid=search_params, \n",
    "                      scoring=cv_silhouette_scorer, cv=cv, n_jobs=-1)\n",
    "\n",
    "    res = gs.fit(X=features, y=None)\n",
    "\n",
    "    # Get best configuration\n",
    "    max_score = np.max(res.cv_results_['mean_test_score'])\n",
    "    ind = np.argmax(res.cv_results_['mean_test_score'])\n",
    "    best_params = res.cv_results_['params'][ind]\n",
    "    k = best_params['min_samples']\n",
    "    # metric = best_params['metric']\n",
    "\n",
    "    # Visualize best clusters\n",
    "    print(f\"Best validation score {max_score:.3f} achieved with {res.cv_results_['params'][ind]}\")\n",
    "    optics_estimator = OPTICS(cluster_method='xi', min_samples=k)\n",
    "    optics_name = optics_estimator.__class__.__name__\n",
    "\n",
    "    optics_clusters = optics_estimator.fit(features)\n",
    "    optics_metrics = calculate_metrics(optics_estimator, df, features, optics_clusters.labels_[optics_clusters.ordering_])\n",
    "    optics_results[key] = {'labels': optics_clusters.labels_[optics_clusters.ordering_], 'metrics': optics_metrics}\n",
    "    \n",
    "    estimators[f\"{optics_name} on {key}\"] = {'estimator': optics_estimator, 'score':  max_score, 'method representation': key }\n",
    "    estimator_metrics[f\"{optics_name} on {key}\"] = {'Method Representation': key, 'Estimator': optics_name, 'Silhouette Score': optics_metrics['silhouette_score'], 'Dunn Index': optics_metrics['dunn_index'], 'Adjusted Rand Index': optics_metrics['adjusted_rand_score']}\n",
    "    \n",
    "plot_clusters(optics_clusters, optics_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agglomerative Clustering (Hierarchical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "search_params = {'n_clusters': np.arange(2,11), 'linkage': ['ward', 'complete', 'average', 'single']}\n",
    "# search_params = {'n_clusters': np.arange(2,11), 'linkage': ['complete', 'average', 'single'], 'affinity': ['euclidean', 'l1', 'l2', 'manhattan', 'cosine']}\n",
    "agglomerative_results = {}\n",
    "\n",
    "for key in datasets:\n",
    "    df = datasets[key]['DataFrame']\n",
    "    features = datasets[key]['Features']\n",
    "    \n",
    "    cv = [(slice(None), slice(None))] # Disable cv, only want grid search\n",
    "    gs = GridSearchCV(estimator=AgglomerativeClustering(linkage='ward'), param_grid=search_params, \n",
    "                      scoring=cv_silhouette_scorer, cv=cv, n_jobs=-1)\n",
    "\n",
    "    res = gs.fit(X=features, y=None)\n",
    "\n",
    "    # Get best configuration\n",
    "    max_score = np.max(res.cv_results_['mean_test_score'])\n",
    "    ind = np.argmax(res.cv_results_['mean_test_score'])\n",
    "    best_params = res.cv_results_['params'][ind]\n",
    "    k = best_params['n_clusters']\n",
    "    linkage = best_params['linkage']\n",
    "\n",
    "    # Visualize best clusters\n",
    "    print(f\"Best validation score {max_score:.3f} achieved with {res.cv_results_['params'][ind]}\")\n",
    "    agglomerative_estimator = AgglomerativeClustering(n_clusters=k, linkage=linkage)\n",
    "    agglomerative_name = agglomerative_estimator.__class__.__name__\n",
    "\n",
    "    agglomerative_clusters = agglomerative_estimator.fit(features)\n",
    "    agglomerative_metrics = calculate_metrics(agglomerative_clusters, df, features, agglomerative_clusters.labels_)\n",
    "    \n",
    "    agglomerative_results[key] = {'labels': agglomerative_clusters.labels_, 'metrics': agglomerative_metrics}\n",
    "    \n",
    "    estimators[f\"{agglomerative_name} on {key}\"] = {'estimator': agglomerative_estimator, 'score':  max_score, 'method representation': key}\n",
    "    estimator_metrics[f\"{agglomerative_name} on {key}\"] = {'Method Representation': key, 'Estimator': agglomerative_name, 'Dunn Index': agglomerative_metrics['dunn_index'], 'Silhouette Score': agglomerative_metrics['silhouette_score'], 'Adjusted Rand Index': agglomerative_metrics['adjusted_rand_score']}\n",
    "    \n",
    "plot_clusters(agglomerative_clusters, agglomerative_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select best estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores = [val['score'] for val in estimators.values()]\n",
    "ind = np.argmax(scores)\n",
    "best_estimator = list(estimators.values())[0]['estimator']\n",
    "best_method_representation = list(estimators.values())[0]['method representation']\n",
    "print(f\"Best cluster method and representation: {best_estimator.__class__.__name__} on {best_method_representation}\\nParams:\\n{best_estimator.__dict__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write configs of the best estimators from each category to file\n",
    "with open(f\"./results/code2vec_{DATASET_NAME}_estimators_config.txt\", \"w\") as writer:\n",
    "    for conf, score in [(val['estimator'].__dict__,val['score']) for val in estimators.values()]:\n",
    "        writer.write(f\"Estimator config:\\n{conf}\\nSilhouette Score: {score}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Write the chosen cluster metrics for the best models to table\n",
    "latex_table = pd.DataFrame([d for d in estimator_metrics.values()], columns=['Method Representation','Estimator','Dunn Index', 'Silhouette Score', 'Adjusted Rand Index']).to_latex(index=False, float_format=\"%.3f\").replace('\\\\toprule', '\\\\hline').replace('\\\\midrule', '\\\\hline').replace('\\\\bottomrule','\\\\hline')\n",
    "with open(f\"./results/tables/{DATASET_NAME}/code2vec_{DATASET_NAME}_table.tex\", \"w\") as writer:\n",
    "    writer.write(latex_table)\n",
    "\n",
    "# Write all cluster metrics to table\n",
    "latex_table = pd.DataFrame([d for d in [kmeans_metrics, optics_metrics, agglomerative_metrics]]).to_latex(index=False, float_format=\"%.3f\").replace('\\\\toprule', '\\\\hline').replace('\\\\midrule', '\\\\hline').replace('\\\\bottomrule','\\\\hline')\n",
    "with open(f\"./results/tables/{DATASET_NAME}/code2vec_{DATASET_NAME}_all_metrics_table.tex\", \"w\") as writer:\n",
    "    writer.write(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Clusters with Kepler Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize with kepler map\n",
    "mapper = km.KeplerMapper(verbose=1)\n",
    "\n",
    "# Fit and transform data, use TSNE \n",
    "projected_data = mapper.fit_transform(datasets[best_method_representation]['Features'], projection=TSNE(n_components=2))\n",
    "\n",
    "# Create the graph (we cluster on the projected data and suffer projection loss)\n",
    "graph = mapper.map(\n",
    "    projected_data,\n",
    "    clusterer=best_estimator,\n",
    "    cover=km.Cover(5, 0.3),\n",
    ")\n",
    "\n",
    "# Create the visualizations\n",
    "print(\"Output graph examples to html\")\n",
    "mapper.visualize(\n",
    "    graph,\n",
    "    title=f\"{DATASET_NAME} {best_method_representation} Mapper\",\n",
    "    path_html=f\"./results/figures/{DATASET_NAME}/{DATASET_NAME}_visualization.html\",\n",
    "    custom_tooltips=datasets[best_method_representation]['DataFrame']['category'].values\n",
    ")\n",
    "\n",
    "jupyter.display(path_html=f\"./results/figures/{DATASET_NAME}/{DATASET_NAME}_visualization.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (scc)",
   "language": "python",
   "name": "scc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
